{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is a model? Example with spam classification:\n",
    "- If only 1% of emails are spam and we build a classifier that predicts all emails as real, its accuracy is 99%!\n",
    "- This is because of class imbalance, when one class if more frequent.\n",
    "To diagnose classification predictions we use confusion matrix\n",
    "- true positive --> predicted: spam email & actual: spam\n",
    "- False positive --> predicted: spam & actual: real\n",
    "- false negative --> predicted: real & actual: spam..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class of interest is the positive class. Since we're trying to classify spam, those will be the positive. \n",
    "We can retrieve accuracy and other metrics from the confusion matrix:\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$Accuracy = \\frac{t_p + t_n}{t_p + t_n + f_p + f_n }$ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$Accuracy = \\frac{t_p + t_n}{t_p + t_n + f_p + f_n }$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$ppv = Precision = \\frac{t_p}{t_p + f_p}$ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$ppv = Precision = \\frac{t_p}{t_p + f_p}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$sensitiviy = hit rate = recall = \\frac{t_p}{t_p + f_n}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$sensitiviy = hit rate = recall = \\frac{t_p}{t_p + f_n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score = 2 * precision*recall / (precision+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High precision = not many real emails predicted as spam\n",
    "- High recall = predicted most spam emails correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KNeighborsClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-779689144a07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KNeighborsClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "#To do stuff in scikit-learn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn = KNeighborsClassifier(n_neighbors = 8)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 42)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)\n",
    "\n",
    "#to compute confusion matrix\n",
    "#the true label is always the first argument\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#to get relevant metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "#Note the column support. The support gives the number of samples of the true response that lie in that class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example using PIMA Indians diabetes set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\rodri\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\rodri\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "Pregnancies                 768 non-null int64\n",
      "Glucose                     768 non-null int64\n",
      "BloodPressure               768 non-null int64\n",
      "SkinThickness               768 non-null int64\n",
      "Insulin                     768 non-null int64\n",
      "BMI                         768 non-null float64\n",
      "DiabetesPedigreeFunction    768 non-null float64\n",
      "Age                         768 non-null int64\n",
      "Outcome                     768 non-null int64\n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if we have missing values\n",
    "np.NaN in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176  30]\n",
      " [ 56  46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80       206\n",
      "           1       0.61      0.45      0.52       102\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       308\n",
      "   macro avg       0.68      0.65      0.66       308\n",
      "weighted avg       0.71      0.72      0.71       308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = df.drop('Outcome', axis = 1).values\n",
    "y = df['Outcome'].values\n",
    "# Create training and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.4, random_state=42)\n",
    "\n",
    "# Instantiate a k-NN classifier: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(x_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite name, we'll use this one on classification problems, not regression ones. \n",
    "\n",
    "Here we will tackle binary classification (two possible labels for target).\n",
    "\n",
    "Logistic regression outputs probabilities:\n",
    "- p > 0.5 --> 1\n",
    "- p < 0.5 --> 0\n",
    "LogReg produces a linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodri\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\rodri\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.4, random_state=42)\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default our decision probability threshold is 0.5.\n",
    "\n",
    "When we have a threshold of 0 all outputs are 1 and we have same rate for true positive and false positives = 1\n",
    "\n",
    "Same result when the thresh. is equal to 1.\n",
    "\n",
    "For intermediate values we get different points in the plot False Positive rate VS True positive rate (Also called ROC curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To plot ROC curve\n",
    "# ROC curves provide a way to visually evaluate models\n",
    "from sklearn.metrics import roc_curve\n",
    "#the following method returns an array with two columns. We select the second one\n",
    "#That corresponds to the probability of the predicted labels being 1\n",
    "y_pred_prob = logreg.predict_proba(x_test)[:,1]\n",
    "#unpacking result\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "#plotting result\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.plot(fpr, tpr, label= 'Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visually evaluate model performance is to plot the precision VS recall curve.\n",
    "\n",
    "Now that we have the ROC curve, which metric can we get from it?\n",
    "- The larger the area under the ROC curve (AUC) the better our model is!\n",
    "- The ideal case would be to have a point at (0,1) corresponding to Tpr = 1 and Fpr = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8266228821625738"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing the AUC\n",
    "#In this case we'll be using logreg as our classifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "logreg = LogisticRegression()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state=42)\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "#first we compute the predicted prob\n",
    "y_pred_prob= logreg.predict_proba(x_test)[:,1]\n",
    "\n",
    "roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also compute AUC using cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(logreg, x, y, cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7987037  0.80777778 0.81962963 0.86622642 0.85037736]\n"
     ]
    }
   ],
   "source": [
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC computation\n",
    "Say you have a binary classifier that in fact is just randomly making guesses. It would be correct approximately 50% of the time, and the resulting ROC curve would be a diagonal line in which the True Positive Rate and False Positive Rate are always equal. The Area under this ROC curve would be 0.5. This is one way in which the AUC, which Hugo discussed in the video, is an informative metric to evaluate a model. If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters cannot be learned by fitting the model. We have to choose them before implementing it.\n",
    "- Linear reg: Choosing parameters\n",
    "- Ridge/Lasso reg: alpha\n",
    "- knn: choosing k\n",
    "The idea will be to try a bunch of different values, fit all of them separately and see how well each performs. Then we can choose the best performing one. \n",
    "\n",
    "It's essential to use cross-validation, otherwise we risk overfitting the test set.\n",
    "\n",
    "A grid search cross-validation is simply a grid with all combinations of parameter values we're gonna try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 14}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#we now specify the hyperparameter in dictionary form\n",
    "#We can see the names of the hyperp in the documentation of each model\n",
    "param_grid = {'n_neighbors':np.arange(1,50)}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv = 5)\n",
    "knn_cv.fit(x,y)\n",
    "\n",
    "#now to check which one performed best\n",
    "knn_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7578125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And what was the best score\n",
    "knn_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice on logreg\n",
    "Hugo demonstrated how to tune the n_neighbors parameter of the KNeighborsClassifier() using GridSearchCV on the voting dataset. You will now practice this yourself, but by using logistic regression on the diabetes dataset instead!\n",
    "\n",
    "Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: C. C controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large C can lead to an overfit model, while a small C can lead to an underfit model.\n",
    "\n",
    "The hyperparameter space for C has been setup for you. Your job is to use GridSearchCV and logistic regression to find the optimal C in this hyperparameter space. The feature array is available as X and target variable array is available as y.\n",
    "\n",
    "You may be wondering why you aren't asked to split the data into training and test sets. Good observation! Here, we want you to focus on the process of setting up the hyperparameter grid and performing grid-search cross-validation. In practice, you will indeed want to hold out a portion of your data for evaluation purposes, and you will learn all about this in the next video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X,y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with RandomizedSearchCV\n",
    "GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. You'll practice using RandomizedSearchCV in this exercise and see how this works.\n",
    "\n",
    "Here, you'll also be introduced to a new model: the Decision Tree. Don't worry about the specifics of how this model works. Just like k-NN, linear regression, and logistic regression, decision trees in scikit-learn have .fit() and .predict() methods that you can use in exactly the same way as before. Decision trees have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf: This makes it an ideal use case for RandomizedSearchCV.\n",
    "\n",
    "As before, the feature array X and target variable array y of the diabetes dataset have been pre-loaded. The hyperparameter settings have been specified for you. Your goal is to use RandomizedSearchCV to find the optimal hyperparameters. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X,y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out set for final evaluation\n",
    "How well can the model perform on never before seen data? Using all data for cv is not idea since we wouldn't be sure that on the selected test that test data was not used. \n",
    "\n",
    "We'll split data into training and hold-out set at the beginning and perform grid search cv on training set\n",
    "\n",
    "After that we can choose the best hyperparameters and evaluate on hold-out set, why has not been used at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out set in practice I: Classification\n",
    "You will now practice evaluating a model with tuned hyperparameters on a hold-out set. The feature array and target variable array from the diabetes dataset have been pre-loaded as X and y.\n",
    "\n",
    "In addition to C, logistic regression has a 'penalty' hyperparameter which specifies whether to use 'l1' or 'l2' regularization. Your job in this exercise is to create a hold-out set, tune the 'C' and 'penalty' hyperparameters of a logistic regression classifier using GridSearchCV on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Instantiate the logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out set in practice II: Regression\n",
    "Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties:\n",
    "\n",
    "a∗L1+b∗L2\n",
    "In scikit-learn, this term is represented by the 'l1_ratio' parameter: An 'l1_ratio' of 1 corresponds to an L1 penalty, and anything lower is a combination of L1 and L2.\n",
    "\n",
    "In this exercise, you will GridSearchCV to tune the 'l1_ratio' of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with categorical features\n",
    "Having created the dummy variables from the 'Region' feature, you can build regression models as you did before. Here, you'll use ridge regression to perform 5-fold cross-validation.\n",
    "\n",
    "The feature array X and target variable array y have been pre-loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a ridge regressor: ridge\n",
    "ridge = Ridge(alpha = 0.5, normalize = True)\n",
    "\n",
    "# Perform 5-fold cross-validation: ridge_cv\n",
    "ridge_cv = cross_val_score(ridge, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validated scores\n",
    "print(ridge_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
